{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you are provided with three classes of images, cars, bikes and people in real world settings. You are provided with code for obtaining features for these images (specifically histogram of gradients (HoG) features). You need to implement a boosting based classifier that can be used to classify the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca=PCA(n_components=0.9)\n",
    "\n",
    "#Image feature extraction code\n",
    "def obtain_dataset(folder_name):\n",
    "    # assuming 128x128 size images and HoGDescriptor length of 34020\n",
    "    hog_feature_len=34020\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    \n",
    "    #code for obtaining hog feature for one image file name\n",
    "    files = glob.glob(folder_name+'/*')\n",
    "    #print(files)\n",
    "    X = []\n",
    "    y = []\n",
    "    for i, file in enumerate(files):\n",
    "        images = glob.glob(file+'/*.png')\n",
    "        for image in images:\n",
    "            im = cv2.imread(image)\n",
    "            h = hog.compute(im)\n",
    "            X.append(h[:,0])\n",
    "            y.append(i)\n",
    "            #print(shape(h))\n",
    "    # use this to read all images in the three directories and obtain the set of features X and train labels Y\n",
    "    # you can assume there are three different classes in the image dataset\n",
    "    Xraw = np.array(X)\n",
    "    X = pca.fit_transform(Xraw)\n",
    "    y = np.array(y)\n",
    "    return (X,y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting classifier\n",
    "\n",
    "1. use the previous decison tree to build a decision stump\n",
    "2. modify the decision tree to build a weighted decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_entropy(labels, weights):\n",
    "    n_labels = len(labels)\n",
    "    classes = np.unique(labels)\n",
    "    entropy = 0\n",
    "    for k in classes:\n",
    "        p = np.sum(weights[labels==k])\n",
    "        entropy = entropy - p*np.log(p)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def find_split(X, y, weights):\n",
    "    start_entropy = cal_entropy(y, weights)\n",
    "    best = {'infogain': -np.inf}\n",
    "    N, D = X.shape\n",
    "    #feat_idxs = np.random.choice(D, replace=False)\n",
    "    #loop for each feature\n",
    "    for i in range(D):\n",
    "        #loop of each unique value of each feature\n",
    "        for threshold in np.unique(X[:,i]):\n",
    "            left_indices = np.argwhere(X[:, i] <= threshold).flatten()\n",
    "            right_indices = np.argwhere(X[:, i] > threshold).flatten()\n",
    "            nl = float(len(left_indices))\n",
    "            nr = float(len(right_indices))\n",
    "            n = nl+nr\n",
    "            infogain = start_entropy - nl/n * cal_entropy(y[left_indices], weights[left_indices]) -nr/n *cal_entropy(y[right_indices], weights[right_indices])\n",
    "            #error =  (left_weighted_mistakes + right_weighted_mistakes ) / n\n",
    "            if infogain>best['infogain']:\n",
    "                best={'feature':i,\n",
    "                      'split': threshold,\n",
    "                      'infogain': infogain,\n",
    "                      'left_indices': left_indices,\n",
    "                      'right_indices': right_indices,\n",
    "                     }\n",
    "            #print(left_indices)\n",
    "    return best\n",
    "\n",
    "def build_tree(X, y, weights, max_depth=2):\n",
    "    if max_depth == 1 or (y==y[0]).all():\n",
    "        classes, counts = np.unique(y, return_counts = True)\n",
    "        return {'leaf': True, 'class': classes[np.argmax(counts)]}\n",
    "\n",
    "    else:\n",
    "        move = find_split(X, y, weights)\n",
    "        left = build_tree(X[move['left_indices']], y[move['left_indices']], weights[move['left_indices']], max_depth-1)\n",
    "        right = build_tree(X[move['right_indices']], y[move['right_indices']], weights[move['left_indices']], max_depth-1)\n",
    "        return {'leaf': False,\n",
    "                'feature': move['feature'],\n",
    "                'split': move['split'],\n",
    "                'left': left,\n",
    "                'right': right,\n",
    "                'infogain': move['infogain']\n",
    "               }\n",
    "\n",
    "def predict_one(nodedict, OneX):\n",
    "    if nodedict['leaf']:\n",
    "        return nodedict['class']\n",
    "    else:\n",
    "        if OneX[nodedict['feature']]<=nodedict['split']:\n",
    "            return predict_one(nodedict['left'], OneX)\n",
    "        else:\n",
    "            return predict_one(nodedict['right'], OneX)\n",
    "\n",
    "def predict(tree,Xs):\n",
    "    return [predict_one(tree, X) for X in Xs]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoostingClassifier:\n",
    "\n",
    "    def __init__(self, numBoostingIters=40):\n",
    "        '''\n",
    "        clfs : List object containing individual DecisionTree classifiers, in order of creation during boosting\n",
    "        betas : List of beta values, in order of creation during boosting\n",
    "        '''\n",
    "        self.clfs = []  #list of trees\n",
    "        self.betas = [] #coefficients\n",
    "        self.numBoostingIters = numBoostingIters\n",
    "        #self.maxTreeDepth = maxTreeDepth\n",
    "        self.K = 0 #number of labels\n",
    "        self.classes = []  #labels\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, X, y, random_state=None):\n",
    "        '''\n",
    "        Arguments:\n",
    "            X : n-by-d \n",
    "            y : n-by-1 \n",
    "        '''\n",
    "        self.models = []\n",
    "        self.classes = np.unique((y))\n",
    "        self.K = len(self.classes)\n",
    "        n,d = X.shape\n",
    "        weight = np.full((n,),1/n)\n",
    "        for i in range(self.numBoostingIters):\n",
    "            #***********\n",
    "            #clf = DecisionTreeClassifier(max_depth=self.maxTreeDepth).fit(X,y,sample_weight=weight)\n",
    "            clf=build_tree(X, y, weight)\n",
    "            #*******\n",
    "            prediction = predict(clf, X)\n",
    "            #********\n",
    "            e = 1 - sum(weight[y==prediction])/sum(weight)\n",
    "            \n",
    "            beta = np.log((1-e)/e) + np.log(self.K - 1)\n",
    "            match = prediction==y\n",
    "            weight[~match] *= np.exp(beta)\n",
    "            weight /= weight.sum()\n",
    "            self.clfs.append(clf)\n",
    "            self.betas.append(beta)\n",
    "\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Arguments:\n",
    "            X is an n-by-d ndarray\n",
    "        Returns:\n",
    "            an n-by-1 ndarray of the predictions\n",
    "        '''\n",
    "        n = len(X)\n",
    "        pred = np.zeros((n,self.K))\n",
    "        i = 0\n",
    "        for beta,clf in zip(self.betas, self.clfs):\n",
    "            #yp = predict(clf, X).astype(int)\n",
    "            yp = predict(clf, X)\n",
    "            pred[range(n),yp] += beta\n",
    "            i += 1\n",
    "        pred = np.argmax(pred,axis=1)\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_folder_name='image_dataset'\n",
    "(X_train, Y_train) = obtain_dataset(train_folder_name)\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(X_train, Y_train, train_size=0.8, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.4\n"
     ]
    }
   ],
   "source": [
    "#BoostedDT\n",
    "bc = BoostingClassifier()\n",
    "bc.fit(Xtrain, ytrain)\n",
    "y_pred = bc.predict(Xtest)\n",
    "print('accuracy', accuracy_score(ytest, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.43333333333333335\n"
     ]
    }
   ],
   "source": [
    "#compare with sklearn\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "abc = AdaBoostClassifier(n_estimators=50,\n",
    "                         learning_rate=1)\n",
    "# Train Adaboost Classifer\n",
    "model = abc.fit(Xtrain, ytrain)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "ypred = model.predict(Xtest)\n",
    "print('accuracy', accuracy_score(ytest, ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
